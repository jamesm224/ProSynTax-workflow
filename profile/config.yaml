### Snakemake Specifications ### 
snakefile: workflow/Snakefile
configfile: inputs/config.yaml

rerun-incomplete: True
latency-wait: 120
keep-going: True
keep-incomplete: False

# conda specifications
use-conda: True
conda-frontend: mamba
# conda-prefix: path/to/conda/installations  # useful if you have a previous package installation of the pipeline

# unlock: True  
# dry-run: False  # useful to see


### SLURM Specifications ### 
cluster: 
  mkdir -p logs/{rule} &&
  sbatch
    --partition={resources.partition}
    --ntasks={resources.tasks}
    --cpus-per-task={resources.cpus_per_task}
    --mem={resources.mem}
    --time={resources.time}
    --job-name={rule}-%j
    # specifies how stderr and stdout files are stored
    --output="logs/{rule}/{wildcards}.out"
    --error="logs/{rule}/{wildcards}.err"


### HPC Resource Specifications ### 
# number of jobs/processes/samples running at once 
jobs: 5  

# adjust as needed (increase time and mem if input files are large)
default-resources: 
  - time="1-0"  # default time 
  - partition="define_your_partition"
  - mem=50000  # default memory 
  - cpus_per_task=1  # default cpu: most rules are not multi-threaded
  - tasks=1

# adjust as needed (increase cpus_per_task and mem if input files are large)
set-resources:
  # multi-threaded rules (allocate more cores)
  - run_trim_PE:cpus_per_task=10
  - kaiju_run:cpus_per_task=10
  - blast_reads:cpus_per_task=10

  # memory intensive rules
  - run_trim_PE:mem=100000
  - kaiju_run:mem=100000
  - blast_reads:mem=100000
  - normalize_reads:mem=100000
